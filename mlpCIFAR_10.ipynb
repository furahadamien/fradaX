{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mlpCIFAR-10.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMsoVsUPND7JqXYeu5XpghK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/furahadamien/fradaX/blob/master/mlpCIFAR_10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BLH8ldn7o8mf",
        "colab_type": "code",
        "outputId": "42cc3526-b6b5-4fdd-9a93-9ea3470f2ada",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import pickle as pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import math\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import datasets, linear_model\n",
        "import scipy.sparse\n",
        "import copy\n",
        "import random\n",
        "import matplotlib \n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def load_data(dir, train_set=49000, validation_set=1000, test_set=1000):\n",
        "    X_train, y_train, X_test, y_test = load(dir) #loading data\n",
        "\n",
        "    # Subsample the data\n",
        "    mask = range(train_set, train_set + validation_set)\n",
        "    X_val = X_train[mask]\n",
        "    y_val = y_train[mask]\n",
        "    mask = range(train_set)\n",
        "    X_train = X_train[mask]\n",
        "    y_train = y_train[mask]\n",
        "    mask = range(test_set)\n",
        "    X_test = X_test[mask]\n",
        "    y_test = y_test[mask]\n",
        "\n",
        "    X_train = X_train.astype(np.float64)\n",
        "    X_val = X_val.astype(np.float64)\n",
        "    X_test = X_test.astype(np.float64)\n",
        "\n",
        "    \"\"\"\n",
        "    we tanspose the samples so that the channels come first\n",
        "    \"\"\"\n",
        "    X_train = X_train.transpose(0, 3, 1, 2)\n",
        "    X_val = X_val.transpose(0, 3, 1, 2)\n",
        "    X_test = X_test.transpose(0, 3, 1, 2)\n",
        "    image = np.mean(X_train, axis=0)\n",
        "    std_dev = np.std(X_train)\n",
        "\n",
        "    X_train -= image\n",
        "    X_val -= image\n",
        "    X_test -= image\n",
        "\n",
        "    X_train /= std_dev\n",
        "    X_val /= std_dev\n",
        "    X_test /= std_dev\n",
        "    return {\n",
        "        'X_train': X_train, 'y_train': y_train,\n",
        "        'X_val': X_val, 'y_val': y_val,\n",
        "        'X_test': X_test, 'y_test': y_test,\n",
        "        'mean': image, 'std': std_dev\n",
        "    }\n",
        "\n",
        "\"\"\"\n",
        "Load bacthes of the data downloaded from https://www.cs.toronto.edu/~kriz/cifar.html\n",
        "image data is already split into batches that we use for our experiment\n",
        "\"\"\"\n",
        "def get_batches(filename):\n",
        "    with open(filename, 'rb') as f:\n",
        "        images = pickle.load(f, encoding ='bytes') # encoding into byte data\n",
        "        X = images[b'data']\n",
        "        Y = images[b'labels']\n",
        "        #reshape the image array\n",
        "        X = X.reshape(10000, 3, 32, 32).transpose(0, 2, 3, 1)\n",
        "        Y = np.array(Y)\n",
        "        return X, Y\n",
        "\n",
        "#gets data from from the root directory.\n",
        "#full CIFAR-10 data set should be downloaded into the project root directory\n",
        "def get_data(root_dir):\n",
        "    xs = []\n",
        "    ys = []\n",
        "    for b in range(1, 6):\n",
        "        #data is already split into training and testing batches\n",
        "        f = os.path.join(root_dir, 'data_batch_%d' % (b, ))\n",
        "        X, Y = get_batches(f)\n",
        "        xs.append(X)\n",
        "        ys.append(Y)\n",
        "    Xtr = np.concatenate(xs)\n",
        "    Ytr = np.concatenate(ys)\n",
        "    del X, Y\n",
        "    \n",
        "    #get testing batch\n",
        "    Xte, Yte = get_batches(os.path.join(root_dir, 'test_batch'))\n",
        "    return Xtr, Xte, Ytr, Yte\n",
        "\n",
        "def load(file_name):\n",
        "        train_x, test_x, train_y, test_y = get_data(\"\")\n",
        "        # covert N x 3 x 32 x 32 to N x 3072\n",
        "        train_x = np.reshape(train_x, (len(train_x), 3 * 32 * 32))\n",
        "        test_x = np.reshape(test_x, (len(test_x), 3 * 32 * 32))\n",
        "        \n",
        "        return train_x, test_x, train_y, test_y\n",
        "\n",
        "##Cost finctions used in the experiment\n",
        "def sigmoid(z):\n",
        "    return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "\n",
        "def sigmoid_prime(z):\n",
        "    return z * (1 - z)\n",
        "\n",
        "def softmax(z):\n",
        "    z -= np.max(z)\n",
        "    sm = (np.exp(z).T / np.sum(np.exp(z), axis=1)).T\n",
        "    return sm\n",
        "\n",
        "def softmax_prime(z):\n",
        "    return\n",
        "\n",
        "def relu(z):\n",
        "    return np.maximum(z, 0)\n",
        "\n",
        "def relu_prime(z):\n",
        "    dz = np.ones_like(z)\n",
        "    dz[z < 0] = 0\n",
        "    return dz\n",
        "\n",
        "#weight initialization methods\n",
        "def relu_weight(m, n):\n",
        "    np.random.seed(0)\n",
        "    return np.random.rand(m, n) * np.sqrt(2) / np.sqrt(m)\n",
        "\n",
        "def xavier(m, n):\n",
        "    np.random.seed(0)\n",
        "    return np.random.rand(m, n) / np.sqrt(m)\n",
        "\n",
        "def he(m, n):\n",
        "    np.random.seed(0)\n",
        "    return np.random.rand(m, n) * np.sqrt(2) / np.sqrt(m + n)\n",
        "\n",
        "class Layers(object):\n",
        "\n",
        "    def __init__(self, n_in, n_out=10, activation_function=\"relu\"):\n",
        "        self.n_in = n_in\n",
        "        self.n_out = n_out\n",
        "        self.set_activation_functions(act_function_name=activation_function)\n",
        "\n",
        "    def set_activation_functions(self, act_function_name=\"relu\"):\n",
        "        if act_function_name == \"relu\":\n",
        "            self.activation_function = relu\n",
        "            self.function = relu_prime\n",
        "            self.set_weight_function(init_function=\"he\")\n",
        "        elif act_function_name == \"sigmoid\":\n",
        "            self.activation_function = sigmoid\n",
        "            self.function = sigmoid_prime\n",
        "            self.set_weight_function(init_function=\"xavier\")\n",
        "        elif act_function_name == \"softmax\":\n",
        "            self.activation_function = softmax\n",
        "            self.function = softmax_prime\n",
        "            self.set_weight_function(init_function=\"he\")\n",
        "\n",
        "    #weight initialization. \n",
        "    def set_weight_function(self, init_function):\n",
        "        if init_function == \"relu\":\n",
        "            self.weight_function = relu_weight\n",
        "        elif init_function == \"xavier\":\n",
        "            self.weight_function = xavier\n",
        "        elif init_function == \"he\":\n",
        "            self.weight_function = he\n",
        "\n",
        "class Multi_Layer(object):\n",
        "    \n",
        "    def __init__(self, n_in=784, n_out=10, l_rate=0.01):\n",
        "\n",
        "        self.n_in = n_in\n",
        "        self.n_out = n_out\n",
        "        self.initial_lrate = l_rate\n",
        "        self.l_rate = l_rate\n",
        "\n",
        "        self.weights = []\n",
        "        self.biases = []\n",
        "        self.previous_weights = []\n",
        "        self.previous_biases = []\n",
        "        self.layers = []\n",
        "        self.losses = []\n",
        "        \n",
        "        \n",
        "\n",
        "    def layer(self, activation_function=\"relu\", num_perceptrons=4):\n",
        "        if len(self.layers) <= 0:\n",
        "            perceptrons = self.n_in\n",
        "        else:\n",
        "            perceptrons = self.layers[-1].n_out\n",
        "\n",
        "        L = Layers(n_in=perceptrons, n_out=num_perceptrons, activation_function=activation_function)\n",
        "        self.layers.append(L)\n",
        "        \n",
        "    \n",
        "    def backprop_weights(self):\n",
        "        self.previous_weights.append(self.weights)\n",
        "        self.previous_biases.append(self.biases)\n",
        "    \n",
        "    \n",
        "    def forward(self, X):\n",
        "        a = [X]\n",
        "        for l in range(len(self.layers)):\n",
        "            z = a[l].dot(self.weights[l]) + self.biases[l]\n",
        "            activation = self.layers[l].activation_function(z)\n",
        "            a.append(activation)\n",
        "\n",
        "        return a\n",
        "    \n",
        "    \"\"\"\n",
        "    Backpropagation method. Computes the gradient through previous layers and it trains\n",
        "    uses the compuatation to update the weights of the non-linearity\n",
        "    \"\"\"\n",
        "    def backpropagation(self, x,y_mat,a):\n",
        "        m = x.shape[0]\n",
        "        output = a[-1]\n",
        "        \n",
        "        #Computes loss\n",
        "        loss = (-1 / m) * np.sum(y_mat * np.log(output))\n",
        "        deltas = []\n",
        "        delta = y_mat - output\n",
        "        deltas.append(delta)\n",
        "        \n",
        "        for l in range(len(self.layers)-1):\n",
        "            prime = self.layers[-2 - l].function(a[-2 - l])\n",
        "            w = self.weights[-1-l]\n",
        "            delta = np.dot(delta, w.T) * prime\n",
        "            deltas.append(delta)\n",
        "        \n",
        "        prev_weights = self.previous_weights.pop(0)\n",
        "        prev_biases = self.previous_biases.pop(0)\n",
        "\n",
        "        #Stochastic Gradient Descent part\n",
        "        #loss gradients w.r.t input\n",
        "        for l in range(len(self.layers)-1):\n",
        "            dw = (2/m) * np.dot(a[l].T,deltas[-1-l])\n",
        "            self.weights[l] += self.l_rate * dw\n",
        "            \n",
        "            db = (1/m) * np.sum(deltas[-1-l], axis=0, keepdims=True)\n",
        "            self.biases[l] += self.l_rate * db\n",
        "   \n",
        "        self.backprop_weights()\n",
        "        return loss\n",
        "    \n",
        "    #initialize weights of the non-linearity\n",
        "    # We use Xavier initilaization(above) in our experiment\n",
        "    def weight_init(self):\n",
        "        for i in range(len(self.layers)):\n",
        "            curr_layer = self.layers[i].n_out\n",
        "            prev_layer = self.layers[i].n_in\n",
        "            weights = self.layers[i].weight_function(prev_layer, curr_layer)\n",
        "            self.weights.append(weights)\n",
        "            biases = np.zeros((1, curr_layer))\n",
        "            self.biases.append(biases)\n",
        "\n",
        "    #Training the netork\n",
        "    def train(self, x, y, iteration=100000):\n",
        "        \n",
        "        self.weight_init()\n",
        "        self.backprop_weights()\n",
        "        \n",
        "        y_mat = self.one_hot_encoding(y)\n",
        "        for i in range(iteration):\n",
        "            a = self.forward(x)\n",
        "            loss = self.backpropagation(x,y_mat,a)\n",
        "                       \n",
        "            if i%1000==0:\n",
        "                print('Iteration: {0}  --  Loss: {1}'.format(i,loss))\n",
        "                self.losses.append([i,loss])\n",
        "             \n",
        "\n",
        "    # we perform one hot encoding pre-processing step\n",
        "    def one_hot_encoding(self, Y):\n",
        "        labels = Y.shape[0]\n",
        "        label = scipy.sparse.csr_matrix((np.ones(labels), (Y, np.array(range(labels)))))\n",
        "        label = np.array(label.todense()).T\n",
        "        return label\n",
        "    \n",
        "    def val_predicted(self, x):\n",
        "        probability = self.forward(x)[-1]\n",
        "        preds = np.argmax(probability,axis=1)      \n",
        "        return probability,preds\n",
        "\n",
        "    def get_accuracy(self, x,y):\n",
        "        prob,predicted_val = self.val_predicted(x)\n",
        "        accuracy = sum(predicted_val == y)/(float(len(y)))\n",
        "        percentage = accuracy*100\n",
        "        return percentage\n",
        "    \n",
        "    def plot_loss(self):\n",
        "        errors = np.array(self.losses)\n",
        "        plt.plot(errors[1:, 0], errors[1:, 1], 'r--')\n",
        "        plt.title(\"(CIFAR-10) Loss vs Epoch (Learning rate = 0.01)\")\n",
        "        plt.xlabel(\"Epochs\")\n",
        "        plt.ylabel(\"Loss\")\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "\n",
        "#GET CIFAR-DATA\n",
        "train_x, test_x, train_y, test_y = load(file_name=\"cifar\")\n",
        "\n",
        "#data normalization\n",
        "X_train = train_x[:1000] / 255.\n",
        "y_train = train_y[:1000].astype(int)\n",
        "X_test = test_x[:100] / 255.\n",
        "y_test = test_y[:100].astype(int)\n",
        "\n",
        "n_in = X_train.shape[1]\n",
        "\n",
        "mlp = Multi_Layer(n_in=n_in, n_out=10)\n",
        "\n",
        "\"\"\"\n",
        "for all hidden layers, either ReLU or sigmoid activation functions are used\n",
        "and their eventual perfomance compared\n",
        "for output layer, Softmax is used\n",
        "\"\"\"\n",
        "List = [\"relu\", \"sigmoid\"]\n",
        "activation_func = random.choice(List)\n",
        "mlp.layer(activation_func, num_perceptrons=80)\n",
        "mlp.layer(activation_func, num_perceptrons=30)\n",
        "mlp.layer(activation_function=\"softmax\", num_perceptrons=10)\n",
        "\n",
        "mlp.train(X_train, y_train, iteration=100000)\n",
        "\n",
        "train_accuracy = mlp.get_accuracy(X_train,y_train)\n",
        "test_accuracy = mlp.get_accuracy(X_test,y_test)\n",
        "\n",
        "\n",
        "print('Training Accuracy: {0:0.2f} %'.format(train_accuracy))\n",
        "print('Test Accuracy: {0:0.2f} %'.format(test_accuracy))\n",
        "\n",
        "mlp.plot_loss()\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration: 0  --  Loss: 59.89206769319145\n",
            "Iteration: 1000  --  Loss: 1.8760800311071375\n",
            "Iteration: 2000  --  Loss: 1.6616227678363473\n",
            "Iteration: 3000  --  Loss: 1.4756179573387445\n",
            "Iteration: 4000  --  Loss: 1.2992323948636395\n",
            "Iteration: 5000  --  Loss: 1.1323009730963534\n",
            "Iteration: 6000  --  Loss: 0.9774492605200058\n",
            "Iteration: 7000  --  Loss: 0.8577470592360125\n",
            "Iteration: 8000  --  Loss: 0.719147745026479\n",
            "Iteration: 9000  --  Loss: 0.639167148052773\n",
            "Iteration: 10000  --  Loss: 0.5588514512856371\n",
            "Iteration: 11000  --  Loss: 0.49918294397864643\n",
            "Iteration: 12000  --  Loss: 0.43279083392808854\n",
            "Iteration: 13000  --  Loss: 0.36466884778012015\n",
            "Iteration: 14000  --  Loss: 0.29890183344067417\n",
            "Iteration: 15000  --  Loss: 0.38350652851128053\n",
            "Iteration: 16000  --  Loss: 0.2441613692665127\n",
            "Iteration: 17000  --  Loss: 0.22552452320168803\n",
            "Iteration: 18000  --  Loss: 0.19609644399857398\n",
            "Iteration: 19000  --  Loss: 0.1718400858576789\n",
            "Iteration: 20000  --  Loss: 0.15198691851627594\n",
            "Iteration: 21000  --  Loss: 0.1352706674684705\n",
            "Iteration: 22000  --  Loss: 0.1212072524508659\n",
            "Iteration: 23000  --  Loss: 0.10932488555738744\n",
            "Iteration: 24000  --  Loss: 0.0991663457970727\n",
            "Iteration: 25000  --  Loss: 0.0904060430712197\n",
            "Iteration: 26000  --  Loss: 0.08278447856184093\n",
            "Iteration: 27000  --  Loss: 0.07610449882191594\n",
            "Iteration: 28000  --  Loss: 0.07021838918971082\n",
            "Iteration: 29000  --  Loss: 0.0650202794163386\n",
            "Iteration: 30000  --  Loss: 0.06040661184543922\n",
            "Iteration: 31000  --  Loss: 0.05628789340468728\n",
            "Iteration: 32000  --  Loss: 0.05260373095029408\n",
            "Iteration: 33000  --  Loss: 0.04930319236050887\n",
            "Iteration: 34000  --  Loss: 0.04633281300887472\n",
            "Iteration: 35000  --  Loss: 0.04364591689049748\n",
            "Iteration: 36000  --  Loss: 0.04120715362926851\n",
            "Iteration: 37000  --  Loss: 0.038986493141648496\n",
            "Iteration: 38000  --  Loss: 0.036960179100144375\n",
            "Iteration: 39000  --  Loss: 0.03510566549575748\n",
            "Iteration: 40000  --  Loss: 0.033400198522702366\n",
            "Iteration: 41000  --  Loss: 0.031827215639755074\n",
            "Iteration: 42000  --  Loss: 0.03037913330527052\n",
            "Iteration: 43000  --  Loss: 0.029042142901102446\n",
            "Iteration: 44000  --  Loss: 0.027803947169015537\n",
            "Iteration: 45000  --  Loss: 0.02665283335238723\n",
            "Iteration: 46000  --  Loss: 0.025582325545429507\n",
            "Iteration: 47000  --  Loss: 0.024585731927931686\n",
            "Iteration: 48000  --  Loss: 0.02365432363443674\n",
            "Iteration: 49000  --  Loss: 0.022781926802045726\n",
            "Iteration: 50000  --  Loss: 0.021964178166083433\n",
            "Iteration: 51000  --  Loss: 0.02119560682704105\n",
            "Iteration: 52000  --  Loss: 0.020472880050748522\n",
            "Iteration: 53000  --  Loss: 0.01979334893397724\n",
            "Iteration: 54000  --  Loss: 0.019152332955039274\n",
            "Iteration: 55000  --  Loss: 0.018545416233470734\n",
            "Iteration: 56000  --  Loss: 0.017971309558392053\n",
            "Iteration: 57000  --  Loss: 0.01742801801857196\n",
            "Iteration: 58000  --  Loss: 0.016912924218058196\n",
            "Iteration: 59000  --  Loss: 0.016423743771404768\n",
            "Iteration: 60000  --  Loss: 0.015958619571376384\n",
            "Iteration: 61000  --  Loss: 0.015515832625387941\n",
            "Iteration: 62000  --  Loss: 0.015094223944665674\n",
            "Iteration: 63000  --  Loss: 0.014692343467257996\n",
            "Iteration: 64000  --  Loss: 0.014309050896212646\n",
            "Iteration: 65000  --  Loss: 0.013943241417257682\n",
            "Iteration: 66000  --  Loss: 0.013593978462761055\n",
            "Iteration: 67000  --  Loss: 0.013259932452069944\n",
            "Iteration: 68000  --  Loss: 0.012939959536972257\n",
            "Iteration: 69000  --  Loss: 0.012633175033169783\n",
            "Iteration: 70000  --  Loss: 0.012338899569126335\n",
            "Iteration: 71000  --  Loss: 0.012056304325691193\n",
            "Iteration: 72000  --  Loss: 0.011784756215334612\n",
            "Iteration: 73000  --  Loss: 0.01152395918461373\n",
            "Iteration: 74000  --  Loss: 0.011273569239094942\n",
            "Iteration: 75000  --  Loss: 0.011032836070983891\n",
            "Iteration: 76000  --  Loss: 0.010801155156710334\n",
            "Iteration: 77000  --  Loss: 0.010578118634389283\n",
            "Iteration: 78000  --  Loss: 0.010363127042970747\n",
            "Iteration: 79000  --  Loss: 0.01015580668347675\n",
            "Iteration: 80000  --  Loss: 0.009955621913457087\n",
            "Iteration: 81000  --  Loss: 0.009762470038172868\n",
            "Iteration: 82000  --  Loss: 0.009575968613635534\n",
            "Iteration: 83000  --  Loss: 0.00939574447596384\n",
            "Iteration: 84000  --  Loss: 0.009221524974116569\n",
            "Iteration: 85000  --  Loss: 0.009052845517901642\n",
            "Iteration: 86000  --  Loss: 0.008889636440539634\n",
            "Iteration: 87000  --  Loss: 0.008731584005802305\n",
            "Iteration: 88000  --  Loss: 0.00857857311551168\n",
            "Iteration: 89000  --  Loss: 0.008430227666948018\n",
            "Iteration: 90000  --  Loss: 0.008286340484176797\n",
            "Iteration: 91000  --  Loss: 0.008146783802992412\n",
            "Iteration: 92000  --  Loss: 0.008011401044947563\n",
            "Iteration: 93000  --  Loss: 0.007879991131737266\n",
            "Iteration: 94000  --  Loss: 0.00775237532999238\n",
            "Iteration: 95000  --  Loss: 0.007628376749228765\n",
            "Iteration: 96000  --  Loss: 0.007507772727179332\n",
            "Iteration: 97000  --  Loss: 0.007390495657385852\n",
            "Iteration: 98000  --  Loss: 0.007276433737254677\n",
            "Iteration: 99000  --  Loss: 0.007165483183256305\n",
            "Training Accuracy: 100.00 %\n",
            "Test Accuracy: 27.00 %\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}